{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from preprocess import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sets import Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Line   Speaker                                               Text     Date\n",
      "0     1      Holt  Good evening from Hofstra University in Hempst...  9/26/16\n",
      "1     2  Audience                                         (APPLAUSE)  9/26/16\n",
      "2     3   Clinton                               How are you, Donald?  9/26/16\n",
      "3     4  Audience                                         (APPLAUSE)  9/26/16\n",
      "4     5      Holt                                  Good luck to you.  9/26/16\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('debate.csv', encoding = 'iso-8859-1')\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Line  Speaker                                               Text     Date\n",
      "2      3  Clinton                               How are you, Donald?  9/26/16\n",
      "7      8  Clinton  Well, thank you, Lester, and thanks to Hofstra...  9/26/16\n",
      "8      9  Clinton  I also want to see more companies do profit-sh...  9/26/16\n",
      "13    14  Clinton  Well, I think that trade is an important issue...  9/26/16\n",
      "21    22  Clinton  Well, let's stop for a second and remember whe...  9/26/16\n",
      "243\n"
     ]
    }
   ],
   "source": [
    "df_clinton = df[(df['Speaker'] == 'Clinton')]\n",
    "print df_clinton.head()\n",
    "clinton_text = df_clinton['Text'].values.tolist()\n",
    "print len(clinton_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Line Speaker                                               Text     Date\n",
      "10    11   Trump  Thank you, Lester. Our jobs are fleeing the co...  9/26/16\n",
      "11    12   Trump  We cannot let it happen. Under my plan, I'll b...  9/26/16\n",
      "15    16   Trump  Well, for one thing -- and before we start on ...  9/26/16\n",
      "17    18   Trump  Secretary Clinton and others, politicians, sho...  9/26/16\n",
      "19    20   Trump  Well, the first thing you do is don't let the ...  9/26/16\n",
      "355\n"
     ]
    }
   ],
   "source": [
    "df_trump = df[(df['Speaker'] == 'Trump')]\n",
    "print df_trump.head()\n",
    "trump_text = df_trump['Text'].values.tolist()\n",
    "print len(trump_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224\n",
      "1967\n"
     ]
    }
   ],
   "source": [
    "def sentenceTokenize(input_list):\n",
    "    set_lines = Set()\n",
    "    \n",
    "    for line in input_list:\n",
    "        sent_tokenize_list = sent_tokenize(line)\n",
    "        \n",
    "        '''\n",
    "        temp = \"\"\n",
    "        for tt in sent_tokenize_list:\n",
    "            if len(temp) < 140:\n",
    "                temp += tt\n",
    "            else:\n",
    "                set_lines.add(temp)\n",
    "                temp = \"\"\n",
    "        '''\n",
    "        set_lines.update(sent_tokenize_list)\n",
    "       \n",
    "    return set_lines\n",
    "\n",
    "clinton_set = list(sentenceTokenize(clinton_text))\n",
    "trump_set = list(sentenceTokenize(trump_text))\n",
    "\n",
    "tweets = clinton_set + trump_set\n",
    "\n",
    "print len(clinton_set)\n",
    "print len(trump_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_path = '/home/shreya/workspace/AdvanceProject/'\n",
    "#file_out = open(dir_path+'Hillary_thresh_2.txt', 'w')\n",
    "#file_out1 = open(dir_path+'Trump_thresh_2.txt', 'w')\n",
    "\n",
    "file_out = open(dir_path+'Hillary_Debate_orig.txt', 'w')\n",
    "file_out1 = open(dir_path+'Trump_Debate_orig.txt', 'w')\n",
    "\n",
    "for line in clinton_set:\n",
    "    file_out.write(line.encode(\"utf8\")+'\\n!@#$%\\n')\n",
    "\n",
    "for line in trump_set:\n",
    "    file_out1.write(line.encode(\"utf8\")+'\\n!@#$%\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_vecs = vectorizer.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_path = '/home/shreya/workspace/AdvanceProject/'\n",
    "#file_out = open(dir_path+'Hillary_thresh_2.txt', 'w')\n",
    "#file_out1 = open(dir_path+'Trump_thresh_2.txt', 'w')\n",
    "\n",
    "file_out = open(dir_path+'Hillary_Debate2.txt', 'w')\n",
    "file_out1 = open(dir_path+'Trump_Debate2.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "con_start_idx = len(clinton_set)\n",
    "\n",
    "for i in range(0, len(clinton_set)):\n",
    "#for i in random.sample(range(con_start_idx), 5):\n",
    "    cosine_similarities = linear_kernel(tfidf_vecs[i:i+1], tfidf_vecs)\n",
    "    cosine_similarities = cosine_similarities.flatten()\n",
    "    sorted_idx = np.argsort(cosine_similarities)[::-1]\n",
    "    \n",
    "   \n",
    "        \n",
    "    topk_pro = []\n",
    "    topk_con = []\n",
    "\n",
    "    for idx in sorted_idx:\n",
    "        if idx < con_start_idx and idx != i:\n",
    "            if cosine_similarities[idx] >= 0.2:\n",
    "                tweet_score = str(tweets[idx].encode(\"utf8\"))#+\"\\t\"+str(cosine_similarities[idx])\n",
    "                topk_pro.append(tweet_score)\n",
    "                if len(topk_pro) == k:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    for idx in sorted_idx:\n",
    "        if idx >= con_start_idx and idx != i:\n",
    "            if cosine_similarities[idx] >= 0.2:\n",
    "                tweet_score = str(tweets[idx].encode(\"utf8\"))#+\"\\t\"+str(cosine_similarities[idx])\n",
    "                topk_con.append(tweet_score)\n",
    "                if len(topk_con) == k:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    for tweet in topk_con:\n",
    "        file_out.write(str(tweets[i].encode(\"utf8\"))+\"\\n\")\n",
    "        file_out1.write(str(tweet)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
